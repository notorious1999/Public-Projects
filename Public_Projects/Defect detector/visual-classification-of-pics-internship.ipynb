{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom IPython.display import Image\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport pickle\nimport cv2\n\nfrom sklearn.manifold import TSNE\nfrom tensorflow.keras.layers import Conv2D, Activation, BatchNormalization\nfrom tensorflow.keras.layers import UpSampling2D, Input, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.metrics import Recall, Precision, BinaryCrossentropy\nfrom tensorflow.keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I had several projects in my internship in Sweden and one of them was to implement a model able to have a visual classifification of unlabeled defects pictures from the industrial process called additive manufacturing. I leave here a link about the process : https://www.youtube.com/watch?v=t4S0mKjXtT4\n\nThe goal of this notebook is to show you my a little about my work in Sweden. \n\nI had two Dataset of pictures : one from a camera at 90Â° about the tray and an other from a camera based on the optical tomography technology with a little angle this time. Those optical tomography (OT) pictures works as a thermal camera : it is red when the area represents a defect (depending on the threshold used by the operator) and blue when the part does not. I have not the right to use the pictures and the statistics table of all the steps of my work but the goal of this notebook is to show you what I did in Sweden overall.\n\nThe idea here was to use an pre-trained encoder feed by the camera pictures and the OT pictures to decode the defect pictures. The encoder enabled me to extract the main features of the pictures and the decoder to assert if those features extracted are great or not. Then, I used TSNE on the output from the encoder for visualization in order to see if there is some clusters or not and more important, if each cluster represents a particular defect. Hopefully, the result was great (each dot represent a picture) and checked by the experts of additive manufacturing :"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/results/Capture.PNG\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing of the pictures\n\n**The issues here is to import and crop the pictures. Then, we will have to focus the pictures on the defected area. For that, I will use the work I had done before this mission which was to define the defects with some statistics. One of those statistics are the surface and the boundaries of the defected area. Knowing that, it is simpler to succeed on this task. Finally, I have to change the perspective of the OT pictures. Previously, I wrote that the OT camera take pictures not perfectly vertically, unlike the \"normal\" camera, and it exists a little angle. I solved this problem by using a perspective matrix. To image my statement, you could make a parallel with the scanner application on your smartphone which change the perspective of your picture to have the document in front of you. I could say that 90% of the difficulty was in this part. In fact, importing the pictures was really tricky because they were spread in different files with different names and for the statistics, sometimes, there was some missing values and columns which was the problem from the API of the 3D printer. You could see below this long work on the preprocessing.**"},{"metadata":{},"cell_type":"markdown","source":"***Importation of the pictures and the statistic tables (for each job). As you can see a lot of work just for importing the pictures... It is due to the fact that every job are on a different file so I decided to create an algorithm to be able any file knowing the singularity of each name job and file.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Importing Jobs (= pictures for one printing)\n\n\n```python\n\nliste_jobs = [\"SI373220200323035837_DEV_Experiment_02_Poor_wettability_20200323_103800\", \"SI373220200318032639_DEV_Experiment_01_STAIRCASE_20200318_094245\", \n         \"SI373220200317041302_DEV_161932_INXM_RADIO_PLATE_R06_20200317_153350\", \"SI373220200316051623_DEV_102002_HARNESS_BRACKET_20200316_120527\",\n        \"SI373220200227021616_DEV_JOB_2_TEST_SPECIMENTS_EOS_ALU_20200218_151555\", \"SI373220200220225638_DEV_811903_AI_DEFORMATION_PRINT_20200220_155231\",\n        \"SI373220200218042613_DEV_JOB_2_TEST_SPECIMENTS_EOS_ALU_20200218_151555\", \"SI373220200210222007_DEV_101916_POROUS_PARAMETERS_DOE_3_20200210_130447\",\n        \"SI373220200206205732_DEV_201971_WARTSILA_CHARGE_AIR_COOLER_STEPO_2020\", \"SI373220200131053019_DEV_Moose_3_20200131_163222\",\n        \"SI373220200115224512_DEV_JOB_2_TEST_SPECIMENS_HOGANAS_20200116_094333\", \"SI373220200113041719_DEV_STORA_ENSO_MESH_MOLD_B03_20200110_141925\",\n        \"SI373220200110054519_DEV_STORA_ENSO_MESH_MOLD_B03_20200110_141925\", \"SI373220200108234130_DEV_JOB_1_TEST_SPECIMENTS_HOGANAS_20200109_103216\",\n        \"SI373220191217060351_DEV_OQTON_AI_DEFORMATION_ANALYSIS_20191217_143050\",\n        \"SI373220191216044632_DEV_101916_POROUS_PARAMETERS_DOE_2_20191216_144507\", \"SI373220191211214844_DEV_MESH_MOLD_B02_20191209_125143\",\n       \"SI373220191206075103_DEV_Mesh_Mold_20191206_154828\",\n        \"SI373220191205053948_DEV_mesh_mold_trial_2_Hoganas_powder_20191205_135905\", \"SI373220191203234047_DEV_MESH_MOLD_CUT_SECTION_TRIAL_1_20191204_093519\",\n        \"SI373220191202041343_161932_Radioplate\", \"SI373220191127080955_111922_Atlascopco\",\n        \"SI373220191111104152_DEV_ABB_Porous_pipe_and_Al_mesh_20191015_132625\"]  \n\nfile_ind = \"C:/Users/Abdelwakil.Benabdi/OneDrive - AMEXCI AB/clustering/indications Lists/NewIndications/\"\n\n#Importing the OT pictures\n\ndef list_files(directory, extension):\n    return [directory + f for f in os.listdir(directory) if f.endswith('.' + extension)]\notpics = {}\nfor job in liste_jobs:\n    file1 = \"D:/\" + job + \"/\" + job + \"/OT data/\"\n    cast = job.split(\"_\")\n    name_job = cast[0]\n    otpics[name_job] = list_files(file1, \"raw\") #For each job, we add in a list all the OT pictures associated\n    \n#Importing the powder bed pictures\n\npowder_bed = {}\nfor job in liste_jobs:\n    file1 = \"D:/\" + job + \"/\" + job + \"/Powder bed data/\"\n    cast = pic.split(\"_\")\n    name_job = cast[0]\n    powder_bed[name_job] = list_files(file1, \"jpg\") #For each job, we add in a list all the camera pictures associated\n    \n#Importing the defect statistics\n\nliste_ind = os.listdir(file_ind)\n\ncol_comma=[\"Job ID\",\"ID OT system\",\"Layer number\",\"layer height\",\"Part ID\",\"Part name\",\"Ind status\",\"DB number\",\"not used\",\"ind start x\",\"ind start y\",\"ind end x\",\"ind end y\",\"ind center x\",\"ind center y\",\"ind area\",\"ind extent\",\"ind mean\",\"ind min\",\"ind max\",\"20\",\"21\"]\ncol_p_comma=[\"Job ID\",\"ID OT system\",\"Layer number\",\"layer height\",\"Part ID\",\"Part name\",\"Ind status\",\"DB number\",\"ind start x\",\"ind start y\",\"ind end x\",\"ind end y\",\"ind center x\",\"ind center y\",\"ind area\",\"ind extent\",\"ind mean\",\"ind min\",\"ind max\"]\ndict_ind = {}\n#We export the statistic tables and here also, there is a little work because the tables are not uniform (empty columns, staggered columns etc.)\nfor ind in liste_ind:\n    try:\n        data_ind = pd.read_csv(file_ind + ind, sep=',', header = None)\n        cast = ind.split(\"_\")\n        ind = cast[0]\n        data_ind.dropna(axis=1, inplace = True)\n        data_ind.columns = col_comma\n        data_ind.drop(labels=[\"not used\", \"20\", \"21\"], axis = 1, inplace = True)\n        dict_ind[ind] = data_ind\n    except:\n        data_ind = pd.read_csv(file_ind + ind, sep=';') #we make the same process as try with a different separator because the file names are not coordinated...\n        cast = ind.split(\"_\")\n        ind = cast[0]\n        data_ind.dropna(axis=1, inplace = True)\n        data_ind.columns = col_comma\n        dict_ind[ind] = data_ind \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I sort now between OT and camera pictures with defects or not accoding to the statistics table which gives us which layer height we have a defect. Also, we have to change the perspective of the powder bed pictures due to their angle difference.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_def_nrs = {}                                                           #dictionary with key = (name of the job + separator + layer height) and value = pictures associated (OT pictures + two powder bed pictures) \npoints1 = np.float32([[[0., 0.]], [[0. ,849]], [[749., 0.]], [[749,849 ]]]) #values obtained \npoints2 = np.float32([[225,163],[155, 935], [966, 160], [1020, 935]])              #after some trials compared \nmatrix = cv2.getPerspectiveTransform(pts2, pts1)                            #perspective matrix\ndim = (2000, 2000)                                                          #dimension of our pictures\nfor name_job in dict_ind.keys():                                            #I work job by job\n    data_ind = dict_ind[name_job]\n    data_ind = data_ind[(data_ind[\"Job ID\"] == name_job)]\n    data_ind.sort_values(\"Layer number\", inplace = True)\n    layer_num_def = data_ind[\"Layer number\"].value_counts().to_dict()       #I store the number of defects in each layer\n    just_ot_pics_def = {}                                                   \n    just_pb_pics_def = {}\n    separator = \"_\"\n    #We add the OT pictures in the data_def_nrs dictionnary\n    for picture in otpics[name_job]:\n        cast = picture.split(\"/\")\n        layer_height = int(cast[4].split(\"_\")[1])\n        if layer_height in layer_num_def.keys():\n            ot_pics_def[str(layer_height)] = picture     \n    for layer_def, img in ot_pics_def.items():\n        f = tf.io.read_file(img)\n        x = tf.io.decode_raw(f, out_type=tf.float32, little_endian=True) \n        x = tf.reshape(x, shape=[2000,2000])\n        data_def_nrs[name_job + separator + layer_def] = [x]\n    #One layer has two powder bed pictures (one before the coverage of the tray with powder and an other after the coverage)\n    #so we add for each defect layeder the two pictures\n    count = 0\n    for picture in powder_bed[name_job]:\n        cast = pic.split(\"/\")\n        layer_height = int(cast[4].split(\"_\")[1]) - 1\n        if layer_height in layer_num_def.keys():\n            if count == 0:\n                pb_pics_def[str(layer_height)] = [picture]\n                count = count + 1   \n            if count == 1:\n                pb_pics_def[str(layer_height)].append(picture)\n                count = 0\n    for layer_def, img in pb_pics_def.items():\n        f1 = tf.io.read_file(img[0])\n        f2 = tf.io.read_file(img[1])\n        y = tf.io.decode_image(f1, channels=3, dtype=tf.uint8)\n        y = tf.reshape(y, shape=[1024,1280,3])\n        imWarpColored = cv2.warpPerspective(np.array(y), matrix, (750, 850))   #We change here the perspective of the 1st picture of the powder bed\n        y = cv2.resize(imWarpColored, dim, interpolation = cv2.INTER_AREA)     \n        z = tf.io.decode_image(f2, channels=3, dtype=tf.uint8)\n        z = tf.reshape(z, shape=(1024,1280,3))\n        imWarpColored = cv2.warpPerspective(np.array(z), matrix, (750, 850))   #We change here the perspective of the 2nd picture of the powder bed\n        z = cv2.resize(imWarpColored, dim, interpolation = cv2.INTER_AREA)\n        data_def_nrs[name_job + separator + layer_def].append(y)                #we add here the \n        data_def_nrs[name_job + separator + layer_def].append(z)                #two pictures\nprint(\"finished\" + name_job)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Focusing the pictures on the defected area thanks to the statistic tables. The pictures must have the same size. Originally, the size of the pictures was (2000x2000) and with a histogram, using the defected area values on the statistic tables, I decided to resize at (128x128). The majority of the defected areas are below this \nsize.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"defect_pics = {}    ##ictionary with key = (name of the job + separator + layer height + separator) and \n                    #value = pictures resized associated (OT pictures + two powder bed pictures)\nfor name_job in dict_ind.keys():                                                                       \n    data_ind = dict_ind[name_job]\n    data_ind = data_ind[(data_ind[\"Job ID\"] == name_job)]\n    data_ind.sort_values(\"Layer number\", inplace = True)\n    data_ind[\"diff_x\"] = abs(data_ind[\"ind start x\"] - data_ind[\"ind end x\"])\n    data_ind[\"diff_y\"] = abs(data_ind[\"ind start y\"] - data_ind[\"ind end y\"])\n    layer_num_def = data_ind[\"Layer number\"].value_counts().to_dict()\n    for layer_height, nb_def in layer_num_def.items():\n        count = data_ind[(data_ind[\"Layer number\"] == layer_height)].index\n        for i in range(0, nbdef):                                                          #looping over the number of defects for one layer             \n            defect_pics[name_job + \"_\" + str(layer_height) + \"_\" + str(i)] = []\n            for pic in data_def_nrs[name_job + \"_\" + str(layer_height)]:                   #looping over the three pictures for each defect in one layer i.e. 2 defects in the same layer = 6 pictures (4 powder bed pics + 2 OT pictures)\n                start_x = data_ind[(data_ind[\"Layer number\"] == layer_height)].loc[count[i], \"ind start x\"]\n                start_y = data_ind[(data_ind[\"Layer number\"] == layer_height)].loc[count[i], \"ind start y\"]\n                end_x = data_ind[(data_ind[\"Layer number\"] == layer_height)].loc[count[i], \"ind end x\"]\n                end_y = data_ind[(data_ind[\"Layer number\"] == layer_height)].loc[count[i], \"ind end y\"]\n                lack_x = abs(end_x - start_x)       #lack_x = the width of the resized pictures (= defected area)\n                lack_y = abs(end_y - start_y)       #lack_y = the height of the resized pictures (= defected area)\n                diff_x = abs(128 - lack_x)          #will allow to set the width equals 128\n                diff_y = abs(128 - lack_y)          #will allow to set the height equals 128  \n                if lack_x <= 128 and lack_y <= 128 :   \n                    if diff_x % 2 == 0:\n                        left = start_x - (diff_x // 2)\n                        right = end_x + (diff_x // 2)\n                    else : \n                        left = start_x - (diff_x // 2)\n                        right = end_x + (diff_x // 2) + 1\n                    if diff_y % 2 == 0:\n                        up = start_y - diff_y // 2\n                        down = end_y + diff_y // 2\n                    else : \n                        up = start_y - (diff_y // 2)\n                        down = end_y + (diff_y // 2) + 1\n                    #managing the boundaries of the pictures\n                    if int(left) < 0:\n                        right = 0\n                        left = 128\n                    if int(right) > 2000:\n                        up = 1872\n                        down = 2000\n                    if int(up) > 2000:\n                        left = 1872\n                        right = 2000\n                    if int(down) < 0:\n                        left = 0\n                        right = 128\n                    picture = picture[int(up):int(down), int(left):int(right)]                     #Resizing and focusing on the defected area\n                    defect_pics[name_job + \"_\" + str(layer_height) + \"_\" + str(i)].append(picture)\n                else :\n                    continue","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Saving the data imported and computed on two pickles file in order to earn time (it took me two days to import everything...)***"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(r\"someobject.pickle\", \"wb\") as output_file:\n    pickle.dump(defect_pics, output_file)\n\nwith open(r\"someobject.pickle\", \"rb\") as input_file:\n    defect_pics_load = pickle.load(input_file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Autoencoder Model and Clustering\n\nI used a model called U-Net really used for segmentations in the medical field. To run faster my model, I decided to use a pretrained encoder on the weights of ImageNet (the ImageNet project is a large visual database designed for use in visual object recognition software research). The architecture of U-Net is the following :"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/unet-architecture/u-net-architecture.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The MobileNetv2 archictecture is described below :"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/mobile-net-architecture/MobileNetV2 architecture.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creation of the Dataset for the autoencoder model :"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = []\ntrain_y = []\nfor images in defect_pics_load.values():\n    for image in images:\n        im_preprocessed = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n        train_x.append(im_preprocessed)\n        train_y.append(im_preprocessed)\ndataset = tf.data.Dataset.from_tensor_slices((np.array(train_x), np.array(train_y))).repeat().batch(64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Auto-encoder model**\n\nWe reproduce the decoder part of the U-Net architecture described above. To find the skip connections we could use the model_summary and take all the expand_relu of each blocks corresponding to the architecture. I used sigmoid function because my goal is just to see if we are able to find the area defected has the same shape as the input part even if I could use a softmax function because the pictures are not completely white and black."},{"metadata":{"trusted":true},"cell_type":"code","source":"def autoencoder():\n    inputs = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"input_image\")\n    encoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False, alpha=0.35)\n    skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n    f = [16, 32, 48, 64]\n    x = encoder_output\n    for i in range(1, len(skip_connection_names)+1, 1):\n        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n        x = UpSampling2D((2, 2))(x)\n        x = Concatenate()([x, x_skip])\n        \n        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        \n        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)0\n        \n    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n    x = Activation(\"sigmoid\")(x)\n    \n    model = Model(inputs, x)\n    return model\n\nmodel = autoencoder()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = [BinaryCrossentropy(), Recall(), Precision()]\nmodel.compile(loss=BinaryCrossentropy(), optimizer=tf.keras.optimizers.Nadam(LR), metrics=metrics)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_steps = len(train_x)//BATCH\nif len(train_x) % BATCH != 0:\n    train_steps += 1\n\nmodel.fit(dataset,epochs=100, steps_per_epoch=train_steps)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we could use our model for some predictions and check if the model is able to rebuild the pictures fed by the model by using the encoder ouputs. I tried with the 100th sample (arbitrary) and I found that the image was quite similar to the input picture. So, our goal is almost reached at this point ! "},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('saved_model/autoencoder')\nnew_model = tf.keras.models.load_model('saved_model/autoencoder')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = new_model.predict(np.expand_dims(train_x[100], axis=0))[0] > 0.5\nplt.figure() \nimagesf=[train_x[100],train_y[100],y_pred[:,:,0]]\nfor im in imagesf:\n    plt.figure()\n    plt.imshow(im)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualization part**\n\nWe begin by extracting the output of the encoder. Then, we will use T-SNE to obtain our representation of clusters. "},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"input_image\")\nencoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False, alpha=0.35)\nencoder_output = encoder.get_layer(\"block_13_expand_relu\").output\noutput_encoder_model = tf.keras.Model(inputs=encoder.input, outputs=encoder_output)\nencoder_output_final = output_encoder_model(train_x)\nmodel_tsne = TSNE(n_components = 2, random_state = 0)\ndata = np.array(encoder_output_final[0]).flatten()\ndata = np.reshape(data, (1,12288))\ndata = pd.DataFrame(data)\nfor i in range(1,len(train_x)):\n    encoder_out_np = np.array(encoder_output_final[i])\n    encoder_out = encoder_out_np.flatten()\n    encoder_out = np.reshape(encoder_out, (1,12288))\n    encoder_out = pd.DataFrame(encoder_out)\n    data = pd.concat([data, encoder_out])\ntsne_points = model_tsne.fit_transform(data)\ntsne_df = pd.DataFrame(data = tsne_points, columns=[\"X\",\"Y\"])\nsns.scatterplot(data = tsne_df,x=\"X\",y=\"Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/resultattsne/Capture.PNG\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this representation, we could use a simple classification algorithm to classify our pictures. Thank you very much for your attention ! Feel free to contact me by email : awbenabdi@gmail.com"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}